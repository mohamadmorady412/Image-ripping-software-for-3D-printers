{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author SPaRCNet Code insights + Extra dataset (25 eeg sequences) + Unlabelled data (not clean)\n",
    "From the shared discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.filter import filter_data, notch_filter\n",
    "X2 = X[[0,4,5,6, 11,15,16,17, 0,1,2,3, 11,12,13,14]] - X[[4,5,6,7, 15,16,17,18, 1,2,3,7, 12,13,14,18]]\n",
    "X2 = notch_filter(X2, 200, 60, n_jobs=-1, verbose='ERROR')\n",
    "X2 = filter_data(X2, 200, 0.5, 40, n_jobs=-1, verbose='ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ==> 0.5 to 44 kHz\n",
    "==> Frequencies to notch filter in Hz = 60 ?? ( any idea about 60 used )\n",
    "==> After Difference, applied notch and frequency filters ??\n",
    "Loss Function : WeightedKLDivWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedKLDivWithLogitsLoss(nn.KLDivLoss):\n",
    "    def __init__(self, weight):\n",
    "        super(WeightedKLDivWithLogitsLoss, self).__init__(size_average=None, reduce=None, reduction='none')\n",
    "        self.register_buffer('weight', weight)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # TODO: For KLDivLoss: input should 'log-probability' and target should be 'probability'\n",
    "        # TODO: input for this method is logits, and target is probabilities\n",
    "        batch_size = input.size(0)\n",
    "        log_prob = F.log_softmax(input, 1)\n",
    "        element_loss = super(WeightedKLDivWithLogitsLoss, self).forward(log_prob, target)\n",
    "\n",
    "        sample_loss = torch.sum(element_loss, dim=1)\n",
    "        sample_weight = torch.sum(target * self.weight, dim=1)\n",
    "\n",
    "        weighted_loss = sample_loss*sample_weight\n",
    "        # Average over mini-batch, not element-wise\n",
    "        avg_loss = torch.sum(weighted_loss) / batch_size\n",
    "\n",
    "        return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimiser**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> lr = 6.25*1e-5\n",
    "==> eps = 1.0*1e-8\n",
    "==> betas = (0.9, 0.999)\n",
    "batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model_cnn.parameters(), lr=6.25*1e-5, betas = (0.9,0.999),eps = 1.0*1e-8, weight_decay=1.0*1e-3)\n",
    "batch_size = 32\n",
    "Model\n",
    "class _DenseLayer(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, conv_bias, batch_norm):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        if batch_norm:\n",
    "            self.add_module('norm1', nn.BatchNorm1d(num_input_features)),\n",
    "        # self.add_module('relu1', nn.ReLU()),\n",
    "        self.add_module('elu1', nn.ELU()),\n",
    "        self.add_module('conv1', nn.Conv1d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=conv_bias)),\n",
    "        if batch_norm:\n",
    "            self.add_module('norm2', nn.BatchNorm1d(bn_size * growth_rate)),\n",
    "        # self.add_module('relu2', nn.ReLU()),\n",
    "        self.add_module('elu2', nn.ELU()),\n",
    "        self.add_module('conv2', nn.Conv1d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=conv_bias)),\n",
    "        # self.add_module('conv2', nn.Conv1d(bn_size * growth_rate, growth_rate, kernel_size=7, stride=1, padding=3, bias=conv_bias)),\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"Dense Layer Input: \")\n",
    "        # print(x.size())\n",
    "        new_features = super(_DenseLayer, self).forward(x)\n",
    "        # print(\"Dense Layer Output:\")\n",
    "        # print(new_features.size())\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.Sequential):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, conv_bias, batch_norm):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate, conv_bias, batch_norm)\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features, conv_bias, batch_norm):\n",
    "        super(_Transition, self).__init__()\n",
    "        if batch_norm:\n",
    "            self.add_module('norm', nn.BatchNorm1d(num_input_features))\n",
    "        # self.add_module('relu', nn.ReLU())\n",
    "        self.add_module('elu', nn.ELU())\n",
    "        self.add_module('conv', nn.Conv1d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=conv_bias))\n",
    "        self.add_module('pool', nn.AvgPool1d(kernel_size=2, stride=2))\n",
    "\n",
    "\n",
    "class DenseNetEnconder(nn.Module):\n",
    "    def __init__(self, growth_rate=32, block_config=(4, 4, 4, 4, 4, 4, 4),  #block_config=(6, 12, 24, 48, 24, 20, 16),  #block_config=(6, 12, 24, 16),\n",
    "                 in_channels=16, num_init_features=64, bn_size=4, drop_rate=0.2, conv_bias=True, batch_norm=False):\n",
    "\n",
    "        super(DenseNetEnconder, self).__init__()\n",
    "\n",
    "        # First convolution\n",
    "        first_conv = OrderedDict([('conv0', nn.Conv1d(in_channels, num_init_features, kernel_size=7, stride=2, padding=3, bias=conv_bias))])\n",
    "        # first_conv = OrderedDict([('conv0', nn.Conv1d(in_channels, num_init_features, groups=in_channels, kernel_size=7, stride=2, padding=3, bias=conv_bias))])\n",
    "        # first_conv = OrderedDict([('conv0', nn.Conv1d(in_channels, num_init_features, kernel_size=15, stride=2, padding=7, bias=conv_bias))])\n",
    "\n",
    "        # first_conv = OrderedDict([\n",
    "        #   ('conv0-depth', nn.Conv1d(in_channels, 32, groups=in_channels, kernel_size=7, stride=2, padding=3, bias=conv_bias)),\n",
    "        #   ('conv0-point', nn.Conv1d(32, num_init_features, kernel_size=1, stride=1, bias=conv_bias)),\n",
    "        # ])\n",
    "\n",
    "        if batch_norm:\n",
    "            first_conv['norm0'] = nn.BatchNorm1d(num_init_features)\n",
    "        # first_conv['relu0'] = nn.ReLU()\n",
    "        first_conv['elu0'] = nn.ELU()\n",
    "        first_conv['pool0'] = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.densenet = nn.Sequential(first_conv)\n",
    "\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n",
    "                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate, conv_bias=conv_bias, batch_norm=batch_norm)\n",
    "            self.densenet.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2, conv_bias=conv_bias, batch_norm=batch_norm)\n",
    "                self.densenet.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        # Final batch norm\n",
    "        if batch_norm:\n",
    "            self.densenet.add_module('norm{}'.format(len(block_config) + 1), nn.BatchNorm1d(num_features))\n",
    "        # self.features.add_module('norm5', BatchReNorm1d(num_features))\n",
    "\n",
    "        self.densenet.add_module('relu{}'.format(len(block_config) + 1), nn.ReLU())\n",
    "        self.densenet.add_module('pool{}'.format(len(block_config) + 1), nn.AvgPool1d(kernel_size=7, stride=3))  # stride originally 1\n",
    "\n",
    "        self.num_features = num_features\n",
    "\n",
    "        # Official init from torch repo.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight.data)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.densenet(x)\n",
    "        # print(\"Final Output\")\n",
    "        # print(features.size())\n",
    "        return features.view(features.size(0), -1)\n",
    "\n",
    "\n",
    "class DenseNetClassifier(nn.Module):\n",
    "    # def __init__(self, growth_rate=16, block_config=(3, 6, 12, 8),  #block_config=(6, 12, 24, 48, 24, 20, 16),  #block_config=(6, 12, 24, 16),\n",
    "    #            in_channels=16, num_init_features=32, bn_size=2, drop_rate=0, conv_bias=False, drop_fc=0.5, num_classes=6):\n",
    "    def __init__(self, growth_rate=32, block_config=(4, 4, 4, 4, 4, 4, 4),\n",
    "                 in_channels=16, num_init_features=64, bn_size=4, drop_rate=0.2, conv_bias=True, batch_norm=False, drop_fc=0.5, num_classes=6):\n",
    "\n",
    "        super(DenseNetClassifier, self).__init__()\n",
    "\n",
    "        self.features = DenseNetEnconder(growth_rate=growth_rate, block_config=block_config, in_channels=in_channels,\n",
    "                                         num_init_features=num_init_features, bn_size=bn_size, drop_rate=drop_rate,\n",
    "                                         conv_bias=conv_bias, batch_norm=batch_norm)\n",
    "\n",
    "        # Linear layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=drop_fc),\n",
    "            nn.Linear(self.features.num_features, num_classes)\n",
    "        )\n",
    "\n",
    "        # Official init from torch repo.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight.data)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = self.classifier(features)\n",
    "        return out, features"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
